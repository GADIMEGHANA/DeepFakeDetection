# DeepFake Detection Model
# This code implements a convolutional neural network to detect deepfake images and videos

import os
import json
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.applications import EfficientNetB3
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
from google.colab import drive, files
import cv2
import zipfile
from tqdm import tqdm

# Mount Google Drive
drive.mount('/content/drive')

# Upload Kaggle API credentials
print("Using the kaggle.json you've uploaded")
# Set up kaggle credentials from the uploaded file
os.makedirs('/root/.kaggle', exist_ok=True)
# Check if the file exists with different possible names
uploaded_files = !ls
kaggle_files = [f for f in uploaded_files if "kaggle" in f.lower() and f.endswith(".json")]

if kaggle_files:
    kaggle_file = kaggle_files[0]
    !cp "{kaggle_file}" /root/.kaggle/kaggle.json
    !chmod 600 /root/.kaggle/kaggle.json
    print(f"Using {kaggle_file} for Kaggle authentication")
else:
    print("No kaggle.json file found. Please upload it again.")

# Install kaggle if not installed
!pip install -q kaggle

# Download a more accessible deepfake dataset (Celeb-DF dataset)
print("Downloading alternative deepfake dataset...")
!kaggle datasets download -d manjilpanta/deepfake-detector-1m-faces-224x224

# Create directories to store the dataset
dataset_dir = '/content/deepfake_dataset'
os.makedirs(dataset_dir, exist_ok=True)
os.makedirs(os.path.join(dataset_dir, 'real'), exist_ok=True)
os.makedirs(os.path.join(dataset_dir, 'fake'), exist_ok=True)

# Extract the dataset
print("Extracting dataset...")
try:
    with zipfile.ZipFile('deepfake-detector-1m-faces-224x224.zip', 'r') as zip_ref:
        zip_ref.extractall(dataset_dir)
    print("Dataset extracted successfully")
except FileNotFoundError:
    # Try downloading another dataset if the first one fails
    print("First dataset not found, trying alternative dataset...")
    !kaggle datasets download -d ciplab/real-and-fake-face-detection

    try:
        with zipfile.ZipFile('real-and-fake-face-detection.zip', 'r') as zip_ref:
            zip_ref.extractall(dataset_dir)
        print("Alternative dataset extracted successfully")
    except FileNotFoundError:
        print("Second dataset not found, trying one more option...")
        # Try one more alternative dataset
        !kaggle datasets download -d xhlulu/140k-real-and-fake-faces

        try:
            with zipfile.ZipFile('140k-real-and-fake-faces.zip', 'r') as zip_ref:
                zip_ref.extractall(dataset_dir)
            print("Final alternative dataset extracted successfully")
        except FileNotFoundError:
            print("No dataset could be downloaded. Check your Kaggle API credentials and internet connection.")
            print("We'll proceed with a sample dataset for demonstration.")

            # Create a small sample dataset for demonstration
            sample_dir = '/content/sample_dataset'
            os.makedirs(os.path.join(sample_dir, 'real'), exist_ok=True)
            os.makedirs(os.path.join(sample_dir, 'fake'), exist_ok=True)
            dataset_dir = sample_dir

# Define parameters
IMG_SIZE = 224
BATCH_SIZE = 32
EPOCHS = 20
LEARNING_RATE = 1e-4

# Check dataset structure and adapt accordingly
def check_and_organize_dataset(dataset_dir):
    """Check dataset structure and organize if needed"""
    print(f"Checking dataset structure in {dataset_dir}...")
    subdirs = os.listdir(dataset_dir)
    print(f"Found subdirectories: {subdirs}")

    # Check if dataset is already organized in real/fake structure
    if 'real' in subdirs and 'fake' in subdirs:
        real_files = os.listdir(os.path.join(dataset_dir, 'real'))
        fake_files = os.listdir(os.path.join(dataset_dir, 'fake'))
        if len(real_files) > 0 and len(fake_files) > 0:
            print("Dataset already organized in real/fake structure")
            return

    # Case 1: Dataset has 'training' and 'validation' folders
    if 'training' in subdirs and 'validation' in subdirs:
        print("Found training/validation structure, reorganizing...")
        for split in ['training', 'validation']:
            split_dir = os.path.join(dataset_dir, split)
            classes = os.listdir(split_dir)
            for cls in classes:
                src_dir = os.path.join(split_dir, cls)
                dst_dir = os.path.join(dataset_dir, cls.lower())
                os.makedirs(dst_dir, exist_ok=True)

                # Move files
                for file in os.listdir(src_dir):
                    src_file = os.path.join(src_dir, file)
                    dst_file = os.path.join(dst_dir, f"{split}_{file}")
                    try:
                        os.rename(src_file, dst_file)
                    except:
                        import shutil
                        shutil.copy(src_file, dst_file)

        print("Dataset reorganized to real/fake structure")
        return

    # Case 2: Other dataset structures
    for subdir in subdirs:
        subdir_path = os.path.join(dataset_dir, subdir)
        if os.path.isdir(subdir_path):
            # Guess if it's real or fake based on directory name
            if any(keyword in subdir.lower() for keyword in ['real', 'genuine', 'authentic']):
                target_dir = os.path.join(dataset_dir, 'real')
            elif any(keyword in subdir.lower() for keyword in ['fake', 'deepfake', 'synthetic']):
                target_dir = os.path.join(dataset_dir, 'fake')
            else:
                continue  # Skip if we can't determine

            os.makedirs(target_dir, exist_ok=True)

            # Move files
            for file in os.listdir(subdir_path):
                if file.lower().endswith(('.jpg', '.jpeg', '.png')):
                    src_file = os.path.join(subdir_path, file)
                    dst_file = os.path.join(target_dir, f"{subdir}_{file}")
                    try:
                        os.rename(src_file, dst_file)
                    except:
                        import shutil
                        shutil.copy(src_file, dst_file)

    print("Dataset organized into real/fake structure based on directory names")

# Create sample data if needed (in case no dataset could be downloaded)
def create_sample_dataset(dataset_dir):
    """Create a small sample dataset for demonstration"""
    real_dir = os.path.join(dataset_dir, 'real')
    fake_dir = os.path.join(dataset_dir, 'fake')

    # Check if directories are empty
    if len(os.listdir(real_dir)) > 0 and len(os.listdir(fake_dir)) > 0:
        print("Sample data already exists")
        return

    print("Creating sample dataset for demonstration...")

    # Create random images for demonstration
    for i in range(100):
        # Create a random real image (more structured)
        real_img = np.random.rand(IMG_SIZE, IMG_SIZE, 3) * 255
        # Add some structure to make it look more realistic
        real_img = cv2.GaussianBlur(real_img.astype(np.uint8), (7, 7), 0)
        cv2.imwrite(os.path.join(real_dir, f"sample_real_{i}.jpg"), real_img)

        # Create a random fake image (more noisy)
        fake_img = np.random.rand(IMG_SIZE, IMG_SIZE, 3) * 255
        # Add some artifacts to make it look more fake
        noise = np.random.normal(0, 25, (IMG_SIZE, IMG_SIZE, 3))
        fake_img = np.clip(fake_img + noise, 0, 255).astype(np.uint8)
        cv2.imwrite(os.path.join(fake_dir, f"sample_fake_{i}.jpg"), fake_img)

    print("Created 100 sample real images and 100 sample fake images")

# Call the function to check and organize the dataset
check_and_organize_dataset(dataset_dir)

# Create sample data if needed
real_count = len(os.listdir(os.path.join(dataset_dir, 'real')))
fake_count = len(os.listdir(os.path.join(dataset_dir, 'fake')))

if real_count == 0 or fake_count == 0:
    create_sample_dataset(dataset_dir)
    real_count = len(os.listdir(os.path.join(dataset_dir, 'real')))
    fake_count = len(os.listdir(os.path.join(dataset_dir, 'fake')))

print(f"Dataset contains {real_count} real images and {fake_count} fake images")

# If the dataset is severely imbalanced, balance it
max_samples = min(real_count, fake_count)
max_samples = min(max_samples, 5000)  # Limit to 5000 samples per class to avoid memory issues
if real_count > max_samples * 2 or fake_count > max_samples * 2:
    print("Balancing dataset...")

    # Balance real images
    if real_count > max_samples:
        real_files = os.listdir(os.path.join(dataset_dir, 'real'))
        np.random.shuffle(real_files)
        for file in real_files[max_samples:]:
            os.remove(os.path.join(dataset_dir, 'real', file))

    # Balance fake images
    if fake_count > max_samples:
        fake_files = os.listdir(os.path.join(dataset_dir, 'fake'))
        np.random.shuffle(fake_files)
        for file in fake_files[max_samples:]:
            os.remove(os.path.join(dataset_dir, 'fake', file))

    real_count = len(os.listdir(os.path.join(dataset_dir, 'real')))
    fake_count = len(os.listdir(os.path.join(dataset_dir, 'fake')))
    print(f"Balanced dataset contains {real_count} real images and {fake_count} fake images")

# Data augmentation for training
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest',
    validation_split=0.2
)

# Only rescaling for validation
val_datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2
)

# Load training data
try:
    train_generator = train_datagen.flow_from_directory(
        dataset_dir,
        target_size=(IMG_SIZE, IMG_SIZE),
        batch_size=BATCH_SIZE,
        class_mode='binary',
        subset='training'
    )

    # Load validation data
    validation_generator = val_datagen.flow_from_directory(
        dataset_dir,
        target_size=(IMG_SIZE, IMG_SIZE),
        batch_size=BATCH_SIZE,
        class_mode='binary',
        subset='validation'
    )

    print("Data generators created successfully")
except Exception as e:
    print(f"Error creating data generators: {e}")
    print("Make sure the dataset folder structure is correct with 'real' and 'fake' subfolders.")
    raise

# Make steps_per_epoch and validation_steps calculations safe
steps_per_epoch = max(1, train_generator.samples // BATCH_SIZE)
validation_steps = max(1, validation_generator.samples // BATCH_SIZE)

print(f"Training with {steps_per_epoch} steps per epoch and {validation_steps} validation steps")

# Build the model using transfer learning
def build_model():
    base_model = EfficientNetB3(
        weights='imagenet',
        include_top=False,
        input_shape=(IMG_SIZE, IMG_SIZE, 3)
    )

    # Freeze the base model layers
    base_model.trainable = False

    model = models.Sequential([
        base_model,
        layers.GlobalAveragePooling2D(),
        layers.BatchNormalization(),
        layers.Dense(512, activation='relu'),
        layers.Dropout(0.5),
        layers.Dense(128, activation='relu'),
        layers.Dropout(0.3),
        layers.Dense(1, activation='sigmoid')
    ])

    model.compile(
        optimizer=Adam(learning_rate=LEARNING_RATE),
        loss='binary_crossentropy',
        metrics=['accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]
    )

    return model

# Build the model
model = build_model()
model.summary()

# Set up callbacks
checkpoint = ModelCheckpoint(
    '/content/drive/MyDrive/DeepFakeDetection_checkpoint.h5',
    monitor='val_accuracy',
    save_best_only=True,
    mode='max',
    verbose=1
)

early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True,
    verbose=1
)

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.2,
    patience=3,
    min_lr=1e-6,
    verbose=1
)

# Train the model - first phase
print("Starting initial training phase...")
try:
    history = model.fit(
        train_generator,
        steps_per_epoch=steps_per_epoch,
        validation_data=validation_generator,
        validation_steps=validation_steps,
        epochs=EPOCHS,
        callbacks=[checkpoint, early_stopping, reduce_lr]
    )
    print("Initial training phase completed successfully")
except Exception as e:
    print(f"Error during initial training: {e}")
    # If training fails, we'll still save what we have
    model.save('/content/drive/MyDrive/DeepFakeDetection_partial.h5')
    print("Model saved as 'DeepFakeDetection_partial.h5' despite training errors")
    raise

# Store history metrics for later
initial_history = history.history.copy()

# Fine-tune the model by unfreezing some layers
print("Starting fine-tuning phase...")
base_model = model.layers[0]
# Unfreeze the last few layers
for layer in base_model.layers[-30:]:
    layer.trainable = True

# Recompile with a lower learning rate
model.compile(
    optimizer=Adam(learning_rate=LEARNING_RATE/10),
    loss='binary_crossentropy',
    metrics=['accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]
)

# Continue training with fine-tuning
try:
    fine_tune_history = model.fit(
        train_generator,
        steps_per_epoch=steps_per_epoch,
        validation_data=validation_generator,
        validation_steps=validation_steps,
        epochs=10,
        callbacks=[checkpoint, early_stopping, reduce_lr]
    )
    print("Fine-tuning phase completed successfully")
except Exception as e:
    print(f"Error during fine-tuning: {e}")
    # If fine-tuning fails, we'll still save what we have
    model.save('/content/drive/MyDrive/DeepFakeDetection.h5')
    print("Model saved despite fine-tuning errors")

# Combine training history - handling potential key errors
combined_history = initial_history.copy()
if 'fine_tune_history' in locals():
    for key in fine_tune_history.history:
        # If the key exists in both histories, extend it
        if key in combined_history:
            combined_history[key].extend(fine_tune_history.history[key])
        # If the key only exists in fine_tune_history, add it
        else:
            combined_history[key] = fine_tune_history.history[key]

# Plot training & validation accuracy and loss
plt.figure(figsize=(12, 5))

# Plot accuracy
plt.subplot(1, 2, 1)
plt.plot(combined_history.get('accuracy', []))
plt.plot(combined_history.get('val_accuracy', []))
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Validation'], loc='lower right')

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(combined_history.get('loss', []))
plt.plot(combined_history.get('val_loss', []))
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(['Train', 'Validation'], loc='upper right')

plt.tight_layout()
plt.savefig('/content/drive/MyDrive/DeepFakeDetection_training_curves.png')
plt.show()

# Evaluate the model on validation data
print("Evaluating model on validation data...")
try:
    evaluation = model.evaluate(validation_generator, steps=validation_steps)
    metrics = dict(zip(model.metrics_names, evaluation))

    print(f'Validation Accuracy: {metrics.get("accuracy", "N/A"):.4f}')
    print(f'Validation AUC: {metrics.get("auc", metrics.get("auc_1", "N/A")):.4f}')
    print(f'Validation Precision: {metrics.get("precision", metrics.get("precision_1", "N/A")):.4f}')
    print(f'Validation Recall: {metrics.get("recall", metrics.get("recall_1", "N/A")):.4f}')
except Exception as e:
    print(f"Error during evaluation: {e}")

# Define a function for deepfake detection
def detect_deepfake(model, image_path):
    try:
        img = cv2.imread(image_path)
        if img is None:
            print(f"Error: Cannot read image at {image_path}")
            return "ERROR", 0

        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))
        img = img / 255.0
        img = np.expand_dims(img, axis=0)

        prediction = model.predict(img)[0][0]
        result = "FAKE" if prediction > 0.5 else "REAL"
        confidence = prediction if result == "FAKE" else 1 - prediction

        return result, confidence
    except Exception as e:
        print(f"Error during prediction: {e}")
        return "ERROR", 0

# Save the model to Google Drive
try:
    model.save('/content/drive/MyDrive/DeepFakeDetection.h5')
    print("Model saved as 'DeepFakeDetection.h5' in your Google Drive")
except Exception as e:
    print(f"Error saving model: {e}")
    # Try alternative saving method
    try:
        model.save_weights('/content/drive/MyDrive/DeepFakeDetection_weights.h5')
        print("Model weights saved as 'DeepFakeDetection_weights.h5' in your Google Drive")
    except Exception as e2:
        print(f"Error saving model weights: {e2}")

# Optional: Convert to TensorFlow Lite for mobile deployment
try:
    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    tflite_model = converter.convert()

    with open('/content/drive/MyDrive/DeepFakeDetection.tflite', 'wb') as f:
        f.write(tflite_model)
    print("TensorFlow Lite model saved as 'DeepFakeDetection.tflite' in your Google Drive")
except Exception as e:
    print(f"Error converting to TFLite: {e}")

# Create a simple function to test the model on new images or videos
def test_on_image(image_path):
    result, confidence = detect_deepfake(model, image_path)
    if result == "ERROR":
        print("Could not process the image.")
        return

    print(f"Prediction: {result} with {confidence*100:.2f}% confidence")

    img = cv2.imread(image_path)
    img = cv2.resize(img, (400, 400))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    plt.figure(figsize=(6, 6))
    plt.imshow(img)
    plt.title(f"Prediction: {result} ({confidence*100:.2f}%)")
    plt.axis('off')
    plt.show()

# Function to create a confusion matrix on test data
def create_confusion_matrix():
    # Get a small subset of validation data for testing
    try:
        # Create a separate test generator
        test_datagen = ImageDataGenerator(rescale=1./255)
        test_generator = test_datagen.flow_from_directory(
            dataset_dir,
            target_size=(IMG_SIZE, IMG_SIZE),
            batch_size=1,
            class_mode='binary',
            shuffle=False
        )

        # Limit number of test samples
        num_test_samples = min(100, test_generator.samples)

        # Get predictions
        predictions = []
        true_labels = []

        for i in range(num_test_samples):
            img, label = test_generator.next()
            pred = model.predict(img, verbose=0)[0][0]
            predictions.append(1 if pred > 0.5 else 0)
            true_labels.append(int(label[0]))

        # Create confusion matrix
        cm = confusion_matrix(true_labels, predictions)

        # Plot confusion matrix
        plt.figure(figsize=(8, 6))
        plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
        plt.title('Confusion Matrix')
        plt.colorbar()

        classes = ['Real', 'Fake']
        tick_marks = np.arange(len(classes))
        plt.xticks(tick_marks, classes)
        plt.yticks(tick_marks, classes)

        # Add text annotations
        thresh = cm.max() / 2.
        for i in range(cm.shape[0]):
            for j in range(cm.shape[1]):
                plt.text(j, i, format(cm[i, j], 'd'),
                        horizontalalignment="center",
                        color="white" if cm[i, j] > thresh else "black")

        plt.tight_layout()
        plt.ylabel('True label')
        plt.xlabel('Predicted label')
        plt.savefig('/content/drive/MyDrive/DeepFakeDetection_confusion_matrix.png')
        plt.show()

        # Print classification report
        print("\nClassification Report:")
        print(classification_report(true_labels, predictions, target_names=classes))
    except Exception as e:
        print(f"Error creating confusion matrix: {e}")

# Try to create a confusion matrix
try:
    create_confusion_matrix()
except Exception as e:
    print(f"Could not create confusion matrix: {e}")

print("\nModel training and evaluation complete!")
print("DeepFake Detection model has been saved to your Google Drive as 'DeepFakeDetection.h5'")
print("You can now use this model to detect deepfakes in images and videos.")
print("To test on a new image, use the test_on_image(image_path) function.")

# Example usage (uncomment to use)
# test_on_image('/path/to/your/test/image.jpg')
# Test Data Generator (for final evaluation on unseen data)
test_dir = '/content/test_dataset'  
test_datagen = ImageDataGenerator(rescale=1./255)
test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='binary',
    shuffle=False  # Crucial for maintaining order with predictions
)
